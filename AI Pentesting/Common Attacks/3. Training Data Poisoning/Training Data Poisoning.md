LLMs learn from diverse text. Training data poisoning can lead to user misinformation. Overreliance on AI is a concern. Key data sources include Common Crawl, WebText, OpenWebText, and books.

Training dat poisoning involves compromising the integrity of the LLM's training data, leading to misinformation and biased outputs.

## Attack Vectors

- Control over training data (creating wrong articles for LLM to spider)
- Prompt hidden in training data (Wikipedia for example)
- Access to server and training data stored on server
- Backdooring via additional wrong training

## Examples

### Example 1

An attacker feeds biased or incorrect data into an LLM's training set, causing it to generate inaccurate or harmful outputs, like biased hiring recommendations.

### Example 2

Attacker identifies key data sources like common crawl, WebText, Open Web Text, news, and Wikipedia and creates false articles and documents targeted at this AI. AI model ingests unfiltered/unvetted content.