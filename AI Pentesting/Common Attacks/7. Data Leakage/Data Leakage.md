Data leakage in LLMs can expose sensitive information or proprietary details, leading to privacy and security breaches. Proper data sanitization, and clear terms of use are crucial for prevention.

## Use Cases

Other user B data returned to user A, User A bypasses input filter with prompts and gets user B info, PII leaked into training data.

## Examples

### Example 1

A user manages to get personal information from the LLM, like a user's previous convrrtation details or personal data of another user.

### Example 2

Sensitive info returned by LLM response, including sensitive data in training, sensitive info disclosed due to LLM misinterpretation.

## Step by Step on Enumerating System Info of ChatGPT

![[Pasted image 20240807163228.png]]

![[Pasted image 20240807163258.png]]

![[Pasted image 20240807163404.png]]

![[Pasted image 20240807163530.png]]

![[Pasted image 20240807163634.png]]
