Overreliance: Depending to heavily on LLM for decision-making or content generation.

Overreliance on LLMs can lead to misinformation or inappropriate content do to "hallucinations." Without proper oversight this can result in legal issues and reputational damage.

Note: AI hallucination refers to instances where an artificial intelligence system generates or interprets information that is inaccurate, misleading, or completely fabricated, often due to overfitting, insufficient data, or errors in its learning algorithms.

## Examples

### Example 1

A company relies solely on LLM for hiring, resulting in lack of diversity and potential legal issues to the LLM's unseen biases.

### Example 2

False information, nonsense information, different sources - different answers, outputs too much including PII, risk not communicated.

### Other examples

1. Customer content wrongly generated, misleading news info created because of different sources, harmful content delivered to normal user.

2.
![[Pasted image 20240807210109.png]]

3. Think of the implications on AI/LLM when autonomous vehicles look for red stop signs only……
![[Pasted image 20240807210202.png]]

### Image Scaling Attacks

1. Image inside another image
2. Image loaded by server
3. pre-processing resizes the image
4. results in a different image after pre-processing

https://github.com/EQuiw/2019-scalingattack

### Other Issues

- Training phase (poisoning)
- Querying the model (might predict on different image then user uploaded)
- Attacks behind ML - image bypass filters etc.

### Funny Examples

ChatGPT in March 2024 thinks it’s a dog.
![[Pasted image 20240807210728.png]]

ChatGPT in March 2024 thinks it’s a dog.
![[Pasted image 20240807210758.png]]

ChatGPT in March 2024 thinks it’s money.
![[Pasted image 20240807210829.png]]
