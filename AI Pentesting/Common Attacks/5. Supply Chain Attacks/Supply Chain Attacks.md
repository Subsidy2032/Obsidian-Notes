LLM supply chain attacks risk integrity due to vulnerabilities leading to biases, security breaches, or system failures. Issues arise from pre-trained models, crowdsourced data, and plugin extensions.

As an example, the model might use a public Github repository that might be removed in the future. Another example is accidently cloning a malicious Github repository with a similar name/data as the intended one.

## Examples

### Example 1

A compromised dataset, which is part of the LLM's supply chain, is used for training, leading the model to unintentionally generate outputs that include malware or phishing links.

### Example 2

Vulnerable model used for transfer learning, poisoned crowd sourced data, tampered model or data, 3rd party package vulnerabilities. Examples: OpenGPT, 3P Package exploit, plugin exploits, poisoned PyPi package and get developers to install torjan/backdoor in model zoo (pre-trained model) models.